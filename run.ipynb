{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160a1a9-c5d1-4b61-b403-3957135505cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Args in experiment:\n",
      "数据已是字典格式，直接使用\n",
      "训练集大小:35000,验证集大小:5000,测试集大小:10000\n",
      "Use GPU: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46356/3941442525.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  df_raw = torch.load(os.path.join(args.root_path, args.data_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>start training : DFQRwoI_S3_1_c00_01_data0817.pt_99_[64, 128, 64]>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "tau=0.01\n",
      "Epoch: 1 cost time: 1.1380500793457031\n",
      "Epoch: 1, Steps: 18 | Train Loss: 0.0185974 Vali Loss: 0.0184818 Test Loss: 0.0184273\n",
      "Validation loss decreased (inf --> 0.018482).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 2 cost time: 0.866267204284668\n",
      "Epoch: 2, Steps: 18 | Train Loss: 0.0121809 Vali Loss: 0.0081938 Test Loss: 0.0082107\n",
      "Validation loss decreased (0.018482 --> 0.008194).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 3 cost time: 0.8655307292938232\n",
      "Epoch: 3, Steps: 18 | Train Loss: 0.0089602 Vali Loss: 0.0082886 Test Loss: 0.0083338\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 4 cost time: 0.8775832653045654\n",
      "Epoch: 4, Steps: 18 | Train Loss: 0.0081029 Vali Loss: 0.0080771 Test Loss: 0.0080358\n",
      "Validation loss decreased (0.008194 --> 0.008077).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 5 cost time: 0.8442368507385254\n",
      "Epoch: 5, Steps: 18 | Train Loss: 0.0079787 Vali Loss: 0.0080093 Test Loss: 0.0079360\n",
      "Validation loss decreased (0.008077 --> 0.008009).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 6 cost time: 0.838188648223877\n",
      "Epoch: 6, Steps: 18 | Train Loss: 0.0079546 Vali Loss: 0.0080157 Test Loss: 0.0079428\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 7 cost time: 0.8341059684753418\n",
      "Epoch: 7, Steps: 18 | Train Loss: 0.0079465 Vali Loss: 0.0080923 Test Loss: 0.0080085\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 8 cost time: 0.8411877155303955\n",
      "Epoch: 8, Steps: 18 | Train Loss: 0.0079864 Vali Loss: 0.0081737 Test Loss: 0.0080969\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 9 cost time: 0.8645482063293457\n",
      "Epoch: 9, Steps: 18 | Train Loss: 0.0080420 Vali Loss: 0.0079706 Test Loss: 0.0079089\n",
      "Validation loss decreased (0.008009 --> 0.007971).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 10 cost time: 0.8580424785614014\n",
      "Epoch: 10, Steps: 18 | Train Loss: 0.0079196 Vali Loss: 0.0080519 Test Loss: 0.0079697\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 11 cost time: 0.8677418231964111\n",
      "Epoch: 11, Steps: 18 | Train Loss: 0.0079402 Vali Loss: 0.0081151 Test Loss: 0.0080463\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 12 cost time: 0.8660805225372314\n",
      "Epoch: 12, Steps: 18 | Train Loss: 0.0080040 Vali Loss: 0.0079940 Test Loss: 0.0079230\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 13 cost time: 0.8742012977600098\n",
      "Epoch: 13, Steps: 18 | Train Loss: 0.0079575 Vali Loss: 0.0079036 Test Loss: 0.0078639\n",
      "Validation loss decreased (0.007971 --> 0.007904).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 14 cost time: 0.8830094337463379\n",
      "Epoch: 14, Steps: 18 | Train Loss: 0.0079142 Vali Loss: 0.0080555 Test Loss: 0.0079806\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 15 cost time: 0.8566944599151611\n",
      "Epoch: 15, Steps: 18 | Train Loss: 0.0079053 Vali Loss: 0.0080026 Test Loss: 0.0079518\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 16 cost time: 0.8327422142028809\n",
      "Epoch: 16, Steps: 18 | Train Loss: 0.0079594 Vali Loss: 0.0078968 Test Loss: 0.0078549\n",
      "Validation loss decreased (0.007904 --> 0.007897).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 17 cost time: 0.8316020965576172\n",
      "Epoch: 17, Steps: 18 | Train Loss: 0.0078756 Vali Loss: 0.0078815 Test Loss: 0.0078478\n",
      "Validation loss decreased (0.007897 --> 0.007882).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 18 cost time: 0.8490064144134521\n",
      "Epoch: 18, Steps: 18 | Train Loss: 0.0078587 Vali Loss: 0.0079936 Test Loss: 0.0079330\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 19 cost time: 0.8408713340759277\n",
      "Epoch: 19, Steps: 18 | Train Loss: 0.0079707 Vali Loss: 0.0080444 Test Loss: 0.0079752\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 20 cost time: 0.8893749713897705\n",
      "Epoch: 20, Steps: 18 | Train Loss: 0.0079347 Vali Loss: 0.0078338 Test Loss: 0.0078131\n",
      "Validation loss decreased (0.007882 --> 0.007834).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 21 cost time: 0.8633420467376709\n",
      "Epoch: 21, Steps: 18 | Train Loss: 0.0078545 Vali Loss: 0.0081939 Test Loss: 0.0081624\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 22 cost time: 0.8826415538787842\n",
      "Epoch: 22, Steps: 18 | Train Loss: 0.0079057 Vali Loss: 0.0078870 Test Loss: 0.0078617\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 23 cost time: 0.853492021560669\n",
      "Epoch: 23, Steps: 18 | Train Loss: 0.0078418 Vali Loss: 0.0079151 Test Loss: 0.0078727\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 24 cost time: 0.8729078769683838\n",
      "Epoch: 24, Steps: 18 | Train Loss: 0.0079165 Vali Loss: 0.0078680 Test Loss: 0.0078795\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 25 cost time: 0.8912453651428223\n",
      "Epoch: 25, Steps: 18 | Train Loss: 0.0078437 Vali Loss: 0.0079202 Test Loss: 0.0078843\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : DFQRwoI_S3_1_c00_01_data0817.pt_99_[64, 128, 64]<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tym/notebooks/tym/jiaqi/dfqr/exp/exp_main.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(best_model_path))\n",
      "/home/tym/notebooks/tym/jiaqi/dfqr/exp/exp_main.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(os.path.join('./save_models/', setting + '/' + f\"{self.tau:.2f}\" + '.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.007813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tym/notebooks/tym/jiaqi/dfqr/exp/exp_main.py:173: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(os.path.join('./save_models/', setting + '/' + f\"{self.tau:.2f}\" + '.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DFQRwoI_S3_1_c00_01_data0817.pt_99_[64, 128, 64]>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "tau=0.02\n",
      "Epoch: 1 cost time: 0.8994877338409424\n",
      "Epoch: 1, Steps: 18 | Train Loss: 0.0847050 Vali Loss: 0.0330297 Test Loss: 0.0326299\n",
      "Validation loss decreased (inf --> 0.033030).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 2 cost time: 0.8386642932891846\n",
      "Epoch: 2, Steps: 18 | Train Loss: 0.0311613 Vali Loss: 0.0297863 Test Loss: 0.0296566\n",
      "Validation loss decreased (0.033030 --> 0.029786).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 3 cost time: 0.8307771682739258\n",
      "Epoch: 3, Steps: 18 | Train Loss: 0.0301871 Vali Loss: 0.0291605 Test Loss: 0.0290191\n",
      "Validation loss decreased (0.029786 --> 0.029161).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 4 cost time: 0.8459937572479248\n",
      "Epoch: 4, Steps: 18 | Train Loss: 0.0295277 Vali Loss: 0.0290374 Test Loss: 0.0287841\n",
      "Validation loss decreased (0.029161 --> 0.029037).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 5 cost time: 0.8439121246337891\n",
      "Epoch: 5, Steps: 18 | Train Loss: 0.0288535 Vali Loss: 0.0279875 Test Loss: 0.0277954\n",
      "Validation loss decreased (0.029037 --> 0.027988).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 6 cost time: 0.843275785446167\n",
      "Epoch: 6, Steps: 18 | Train Loss: 0.0280299 Vali Loss: 0.0270056 Test Loss: 0.0269058\n",
      "Validation loss decreased (0.027988 --> 0.027006).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 7 cost time: 0.8427634239196777\n",
      "Epoch: 7, Steps: 18 | Train Loss: 0.0271687 Vali Loss: 0.0263137 Test Loss: 0.0262150\n",
      "Validation loss decreased (0.027006 --> 0.026314).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 8 cost time: 0.8423275947570801\n",
      "Epoch: 8, Steps: 18 | Train Loss: 0.0263887 Vali Loss: 0.0252264 Test Loss: 0.0252324\n",
      "Validation loss decreased (0.026314 --> 0.025226).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 9 cost time: 0.8434658050537109\n",
      "Epoch: 9, Steps: 18 | Train Loss: 0.0253773 Vali Loss: 0.0242998 Test Loss: 0.0244359\n",
      "Validation loss decreased (0.025226 --> 0.024300).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 10 cost time: 0.8438513278961182\n",
      "Epoch: 10, Steps: 18 | Train Loss: 0.0245068 Vali Loss: 0.0235192 Test Loss: 0.0236534\n",
      "Validation loss decreased (0.024300 --> 0.023519).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 11 cost time: 0.8598754405975342\n",
      "Epoch: 11, Steps: 18 | Train Loss: 0.0236819 Vali Loss: 0.0227031 Test Loss: 0.0227820\n",
      "Validation loss decreased (0.023519 --> 0.022703).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 12 cost time: 0.8570556640625\n",
      "Epoch: 12, Steps: 18 | Train Loss: 0.0227988 Vali Loss: 0.0217458 Test Loss: 0.0218795\n",
      "Validation loss decreased (0.022703 --> 0.021746).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 13 cost time: 0.8377256393432617\n",
      "Epoch: 13, Steps: 18 | Train Loss: 0.0219343 Vali Loss: 0.0208738 Test Loss: 0.0212168\n",
      "Validation loss decreased (0.021746 --> 0.020874).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 14 cost time: 0.8479671478271484\n",
      "Epoch: 14, Steps: 18 | Train Loss: 0.0210560 Vali Loss: 0.0198512 Test Loss: 0.0200708\n",
      "Validation loss decreased (0.020874 --> 0.019851).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 15 cost time: 0.8321957588195801\n",
      "Epoch: 15, Steps: 18 | Train Loss: 0.0202398 Vali Loss: 0.0195704 Test Loss: 0.0197811\n",
      "Validation loss decreased (0.019851 --> 0.019570).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 16 cost time: 0.8344886302947998\n",
      "Epoch: 16, Steps: 18 | Train Loss: 0.0196929 Vali Loss: 0.0185954 Test Loss: 0.0187662\n",
      "Validation loss decreased (0.019570 --> 0.018595).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 17 cost time: 0.8408317565917969\n",
      "Epoch: 17, Steps: 18 | Train Loss: 0.0188734 Vali Loss: 0.0181846 Test Loss: 0.0183900\n",
      "Validation loss decreased (0.018595 --> 0.018185).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 18 cost time: 0.8362669944763184\n",
      "Epoch: 18, Steps: 18 | Train Loss: 0.0181989 Vali Loss: 0.0175912 Test Loss: 0.0177389\n",
      "Validation loss decreased (0.018185 --> 0.017591).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 19 cost time: 0.870328426361084\n",
      "Epoch: 19, Steps: 18 | Train Loss: 0.0177354 Vali Loss: 0.0170201 Test Loss: 0.0171861\n",
      "Validation loss decreased (0.017591 --> 0.017020).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 20 cost time: 0.8376705646514893\n",
      "Epoch: 20, Steps: 18 | Train Loss: 0.0171516 Vali Loss: 0.0162562 Test Loss: 0.0164238\n",
      "Validation loss decreased (0.017020 --> 0.016256).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 21 cost time: 0.8372015953063965\n",
      "Epoch: 21, Steps: 18 | Train Loss: 0.0165149 Vali Loss: 0.0159534 Test Loss: 0.0160656\n",
      "Validation loss decreased (0.016256 --> 0.015953).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 22 cost time: 0.8305439949035645\n",
      "Epoch: 22, Steps: 18 | Train Loss: 0.0161503 Vali Loss: 0.0155733 Test Loss: 0.0156424\n",
      "Validation loss decreased (0.015953 --> 0.015573).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 23 cost time: 0.8288934230804443\n",
      "Epoch: 23, Steps: 18 | Train Loss: 0.0161141 Vali Loss: 0.0159703 Test Loss: 0.0159830\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 24 cost time: 0.8414640426635742\n",
      "Epoch: 24, Steps: 18 | Train Loss: 0.0159188 Vali Loss: 0.0150654 Test Loss: 0.0151478\n",
      "Validation loss decreased (0.015573 --> 0.015065).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 25 cost time: 0.8375625610351562\n",
      "Epoch: 25, Steps: 18 | Train Loss: 0.0156123 Vali Loss: 0.0148242 Test Loss: 0.0150419\n",
      "Validation loss decreased (0.015065 --> 0.014824).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 26 cost time: 0.8340139389038086\n",
      "Epoch: 26, Steps: 18 | Train Loss: 0.0153016 Vali Loss: 0.0147795 Test Loss: 0.0149244\n",
      "Validation loss decreased (0.014824 --> 0.014779).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 27 cost time: 0.8416383266448975\n",
      "Epoch: 27, Steps: 18 | Train Loss: 0.0152263 Vali Loss: 0.0147699 Test Loss: 0.0148571\n",
      "Validation loss decreased (0.014779 --> 0.014770).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 28 cost time: 0.8325302600860596\n",
      "Epoch: 28, Steps: 18 | Train Loss: 0.0150572 Vali Loss: 0.0146797 Test Loss: 0.0147457\n",
      "Validation loss decreased (0.014770 --> 0.014680).  Saving model ...\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 29 cost time: 0.8304810523986816\n",
      "Epoch: 29, Steps: 18 | Train Loss: 0.0151297 Vali Loss: 0.0147497 Test Loss: 0.0148377\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 30 cost time: 0.8583076000213623\n",
      "Epoch: 30, Steps: 18 | Train Loss: 0.0149289 Vali Loss: 0.0147069 Test Loss: 0.0147958\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 31 cost time: 0.8342106342315674\n",
      "Epoch: 31, Steps: 18 | Train Loss: 0.0148346 Vali Loss: 0.0146514 Test Loss: 0.0146562\n",
      "Validation loss decreased (0.014680 --> 0.014651).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "Epoch: 32 cost time: 0.8384807109832764\n",
      "Epoch: 32, Steps: 18 | Train Loss: 0.0146810 Vali Loss: 0.0145787 Test Loss: 0.0147039\n",
      "Validation loss decreased (0.014651 --> 0.014579).  Saving model ...\n",
      "Updating learning rate to 0.005\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.backends\n",
    "from exp.exp_main import Exp_main\n",
    "from utils.tools import pad_and_convert_to_dict\n",
    "from utils.tools import save_print\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "fix_seed = 2025\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='DFQR')   #  Deep Functional Quantile Regression with Censoring and Structured Interactions\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--model', type=str, default='DFQRwoI',\n",
    "                    help='model name, options: [DFQR, DFQRwoI]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, default='S3_1_c00_01_data0817', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./dataset/s3/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='S3_1_c00_01_data0817.pt', help='data file')\n",
    "parser.add_argument('--save_path', type=str, default='./save_models/', help='location of saved models')\n",
    "parser.add_argument('--maxlen_function', type=int, default=None, help='max length of functional data')\n",
    "\n",
    "\n",
    "# quantile define\n",
    "parser.add_argument('--num_quantiles', type=int, default=99, help='number of quantiles, e.g. 9 or 99')\n",
    "\n",
    "# model define\n",
    "parser.add_argument('--neurons', type=int, nargs='+', default=[64, 128, 64], help='number of neurons in each layer')\n",
    "parser.add_argument('--dropout', type=float, default=0.01, help='dropout')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--use_norm', type=int, default=1, help='whether to use normalize; True 1 False 0')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--train_epochs', type=int, default=1000, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=2048, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=5, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.01, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='MSE', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--gpu_type', type=str, default='cuda', help='gpu type')  # cuda or mps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#args = parser.parse_args()             #save this in .py file\n",
    "args = parser.parse_args(args=[])       #save this in .ipynb file\n",
    "\n",
    "setting = '{}_{}_{}_{}_{}'.format(\n",
    "    args.model,\n",
    "    args.data,\n",
    "    args.num_quantiles,\n",
    "    args.learning_rate,\n",
    "    args.neurons)\n",
    "save_print(\"./log\", setting)\n",
    "\n",
    "if torch.cuda.is_available() and args.use_gpu:\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu))\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    if hasattr(torch.backends, \"mps\"):\n",
    "        args.device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "    print('Using cpu or mps')\n",
    "\n",
    "args.device = torch.device(\"cuda:0\")     # specify device\n",
    "\n",
    "print('Args in experiment:')\n",
    "#print_args(args)\n",
    "\n",
    "\n",
    "df_raw = torch.load(os.path.join(args.root_path, args.data_path))\n",
    "\n",
    "if isinstance(df_raw, list):\n",
    "    # If the input is a list, perform padding and convert to dictionary\n",
    "    df_raw, final_len = pad_and_convert_to_dict(df_raw, target_len=args.maxlen_function)  # target_len is optional\n",
    "    print(f\"Converted list to dictionary format with uniform length, final length is {final_len}\")\n",
    "else:\n",
    "    print(\"Data is already in dictionary format, using directly\")\n",
    "    \n",
    "n = len(df_raw['X_all']) \n",
    "I_all = torch.ones(n,1)\n",
    "train_size = int(0.7*n)\n",
    "val_size = int(0.1*n)\n",
    "test_size = n - train_size - val_size\n",
    "\n",
    "args.p = df_raw['X_all'].shape[-1]\n",
    "args.q = df_raw['Z_all'].shape[-1]\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(I_all[0:n], df_raw['X_all'][0:n], df_raw['t_all'][0:n], df_raw['Z_all'][0:n], df_raw['Y_all'][0:n], df_raw['C_all'][0:n])\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "test_dataset = Subset(dataset, range(train_size + val_size, n))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=args.batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset,batch_size=args.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=args.batch_size, shuffle=False)\n",
    "print(f\"Train set size: {len(train_dataset)}, Validation set size: {len(val_dataset)}, Test set size: {len(test_dataset)}\")\n",
    "\n",
    "Y_pred_all = torch.zeros((n, 1))\n",
    "for tau in range(args.num_quantiles):\n",
    "    args.tau = (tau + 1) / (args.num_quantiles + 1)\n",
    "    \n",
    "    dataset_pred = torch.utils.data.TensorDataset(Y_pred_all)\n",
    "    train_pred = Subset(dataset_pred, range(0, train_size))\n",
    "    val_pred = Subset(dataset_pred, range(train_size, train_size + val_size))\n",
    "    test_pred = Subset(dataset_pred, range(train_size + val_size, n))\n",
    "\n",
    "    train_loader_pred = DataLoader(train_pred,batch_size=args.batch_size, shuffle=False)\n",
    "    val_loader_pred = DataLoader(val_pred,batch_size=args.batch_size, shuffle=False)\n",
    "    test_loader_pred = DataLoader(test_pred,batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # setting record of experiments\n",
    "    exp = Exp_main(args, train_loader, val_loader, test_loader, \n",
    "        train_loader_pred, val_loader_pred, test_loader_pred)  # set experiments\n",
    "    setting = '{}_{}_{}_{}_{}'.format(\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.num_quantiles,\n",
    "        args.learning_rate,\n",
    "        args.neurons)\n",
    "    \n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    print(f\"tau={args.tau}\")\n",
    "    exp.train(setting)\n",
    "    \n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)\n",
    "    \n",
    "    Y_pred_all = torch.cat([Y_pred_all.cpu(), exp.updateY_pred(setting, I_all, df_raw['X_all'][0:n], df_raw['t_all'][0:n], df_raw['Z_all'][0:n])], dim=1)\n",
    "    \n",
    "    if args.gpu_type == 'mps':\n",
    "        torch.backends.mps.empty_cache()\n",
    "    elif args.gpu_type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(f\"Y_pred_all:{Y_pred_all[0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd6aec-830a-4976-b2b7-0dd8f4514058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
